# Snapshot Tests

When maintaining an `analyzer`, you can use the `batch` binary in order to run
the analyzer on all the fixtures we have.

```bash
bin/batch.sh two-fer
```

The above command generates `analysis.json` for all the `two-fer` fixtures, as
located in `test/fixtures/two-fer/`. You can use these outputs to see if your
analyzer produces the right result.

Once you have established which solutions should generate which status and with
what commentary, it might be a good time to set up a ðŸ“¸ _snapshot_ test. These
will record a snapshot of output and make sure they are the same, every single
time.

## Contents

Add a new file `test/analyzers/<slug>/snapshot.ts` and copy the following
template, replacing `<slug>` with the actual slug and entering **1 to 20** (more
is better, try to have at least 20 per status) fixture numbers to be tested.

```typescript
import { SlugAnalyzer } from '~src/analyzers/<slug>'
import { makeTestGenerator } from '~test/helpers/snapshot'

const snapshotTestsGenerator = makeTestGenerator(
  '<slug>',
  () => new SlugAnalyzer()
)

describe('When running analysis on two-fer fixtures', () => {
  snapshotTestsGenerator('approve', [
    // <fixture numbers>
  ])
  snapshotTestsGenerator('disapprove', [])
  snapshotTestsGenerator('refer_to_mentor', [])
})

```

**Note**: This is not the place to add an exhaustive test of inputs to outputs.
It's merely trying to detect when one of the known cases changes!

## Initial run

After you've added this test file, run the test suite (`yarn test`). It will
generate a snapshot file. The `snapshotTestsGenerator` also validates the output
status, just in case you entered the fixture number in the wrong category.

## Verification and updating

If one of fixtures' output changes, the test suite will fail.

1. Go through each failure manually and validate that it's as expected
1. Run `yarn test -u` to update the snapshots
1. Commit the changed snapshot file (at `test/analyzers/<slug>/__snapshots__/`)
